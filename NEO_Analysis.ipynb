{"cells":[{"cell_type":"code","execution_count":null,"id":"0f11aa0f","metadata":{},"outputs":[],"source":["#>>> Near Earth Object Machine Learning Models by Christopher Madden.\n","\n","#___________________ ¶¶¶¶¶¶¶¶ \n","#_______________¶¶¶¶¶ _______¶¶¶¶¶ \n","#_____________¶¶¶ ________________¶¶¶ \n","#___________¶¶¶ ____________________¶¶¶ \n","#__________¶¶ ________________________¶¶ \n","#_________¶ ______¶¶¶_____¶¶¶__________¶¶ \n","#________¶ _________¶¶______¶¶__________¶¶ \n","#_______¶¶ __________¶¶______¶¶_________¶¶ \n","#_______¶ ____________¶¶______¶¶___¶¶¶___¶¶ \n","#______¶¶ _____¶¶_____¶¶______¶¶_____¶¶__¶¶ \n","#______¶¶ ___¶¶¶______¶¶______¶¶______¶¶_¶¶ \n","#______¶¶ __¶¶¶¶¶__________________¶¶_¶¶_¶¶ \n","#_______¶ __¶¶__¶¶_________________¶¶____¶¶ \n","#_______¶¶ ______¶¶______________¶¶¶____¶¶ \n","#________¶¶ ______¶¶____________¶¶¶_____¶¶ \n","#_________¶¶ _______¶¶¶¶_____¶¶¶¶______¶¶ \n","#__________¶¶ _________¶¶¶¶¶¶¶________¶¶ \n","#____________¶¶ ____________________¶¶ \n","#_____________¶¶¶ ______________¶¶¶ \n","#_______________ ¶¶¶¶¶¶¶¶¶¶¶¶¶¶¶"]},{"cell_type":"markdown","id":"65522264","metadata":{},"source":["# LIBRARIES AND RESOURCES"]},{"cell_type":"code","execution_count":null,"id":"c54f9d2b","metadata":{"id":"c54f9d2b"},"outputs":[],"source":["#>>> Import dependencies.\n","import pandas as pd\n","import tensorflow as tf\n","import numpy as np\n","import datetime\n","import tabpy_clientfrom tabpy.tabpy_tools.client\n","import Client\n","from matplotlib import pyplot as plt\n","from sklearn import tree\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.preprocessing import StandardScaler,OneHotEncoder\n","from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, balanced_accuracy_score, DetCurveDisplay\n","from sklearn.linear_model import LogisticRegressionCV\n","from sklearn.ensemble import VotingClassifier, GradientBoostingClassifier, RandomForestClassifier\n","from sklearn.svm import SVC\n","from autoviz.AutoViz_Class import AutoViz_Class\n","from keras.preprocessing.image import ImageDataGenerator\n","from imblearn.over_sampling import RandomOverSampler\n","from imblearn.combine import SMOTEENN\n","from imblearn.metrics import classification_report_imbalanced\n","from config import db_password\n","from sqlalchemy import create_engine\n","from collections import Counter\n","\n","#>>> Define TabPy client.\n","client = tabpy_client.Client('http://localhost:9004/')\n","\n","#>>> Define database.\n","db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/FINAL_PROJECT\""]},{"cell_type":"markdown","id":"TzrcFSnONyEp","metadata":{"id":"TzrcFSnONyEp"},"source":["# PREPROCESS THE DATASET."]},{"cell_type":"code","execution_count":null,"id":"7Xna75g5NtKT","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":670},"executionInfo":{"elapsed":192,"status":"ok","timestamp":1657897972668,"user":{"displayName":"Christopher Madden","userId":"02171637580083339973"},"user_tz":240},"id":"7Xna75g5NtKT","outputId":"1cdef371-f99e-4559-af74-35c246fb8540"},"outputs":[],"source":["#>>> Extract data from pgAdmin database.\n","engine = create_engine(db_string)\n","neo_df = pd.read_sql_table(table_name='neo', con=engine)\n","\n","#>>> Display the first 10 rows.\n","neo_df.head(10)"]},{"cell_type":"code","execution_count":null,"id":"e8a128eb","metadata":{},"outputs":[],"source":["#>>> Display an overview of the dataset.\n","neo_df.info()"]},{"cell_type":"code","execution_count":null,"id":"EnX94cwXDFDA","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":210,"status":"ok","timestamp":1657897972865,"user":{"displayName":"Christopher Madden","userId":"02171637580083339973"},"user_tz":240},"id":"EnX94cwXDFDA","outputId":"8f9268ce-5b6e-40a0-e8e8-1b2a14f52d10"},"outputs":[],"source":["#>>> Determine the number of unique values in each column.\n","neo_df.nunique()"]},{"cell_type":"code","execution_count":null,"id":"k3MbOM6hHpMw","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1657897972867,"user":{"displayName":"Christopher Madden","userId":"02171637580083339973"},"user_tz":240},"id":"k3MbOM6hHpMw","outputId":"805cb606-3303-4756-a911-1d685f97b047"},"outputs":[],"source":["#>>> Drop unnecessary columns: 'id', 'orbiting_body', 'sentry_object'.\n","neo_df= neo_df.drop(['id', 'orbiting_body', 'sentry_object'],1)\n","\n","#>>> Set index to 'name'.\n","neo_df = neo_df.set_index('name')\n","\n","#>>> Display the first 10 rows.\n","neo_df.head(10)"]},{"cell_type":"code","execution_count":null,"id":"e03194b2","metadata":{},"outputs":[],"source":["#>>> Drop all samples with an absolute magnitude of 22 or greater.\n","neo_df = neo_df[neo_df['absolute_magnitude'] < 22] \n","\n","#>>> Display the first 10 rows.\n","neo_df.head(10)"]},{"cell_type":"code","execution_count":null,"id":"FGUe_xfmHpRM","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":394},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1657897972867,"user":{"displayName":"Christopher Madden","userId":"02171637580083339973"},"user_tz":240},"id":"FGUe_xfmHpRM","outputId":"2696eaed-5326-41a3-8943-f427e3d3e3dc"},"outputs":[],"source":["#>>> Define the features set.\n","X = neo_df.copy()\n","X = X.drop('hazardous', axis=1)\n","\n","#>>> Display the first 10 rows.\n","X.head(10)"]},{"cell_type":"code","execution_count":null,"id":"LYVDmyV9HpUq","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1657897972868,"user":{"displayName":"Christopher Madden","userId":"02171637580083339973"},"user_tz":240},"id":"LYVDmyV9HpUq","outputId":"882b7669-f5ea-4b1d-f1ae-7868daf3b6d7"},"outputs":[],"source":["#>>> Define the target set.\n","y = neo_df['hazardous'].values\n","\n","#>>> Display the first ten values.\n","y[:10]"]},{"cell_type":"code","execution_count":null,"id":"H0ljvXVpLQOp","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":165,"status":"ok","timestamp":1657897973023,"user":{"displayName":"Christopher Madden","userId":"02171637580083339973"},"user_tz":240},"id":"H0ljvXVpLQOp","outputId":"c353fd71-e155-49d7-8243-593a9f1ed462"},"outputs":[],"source":["#>>> Split the preprocessed data into a training and testing dataset.\n","X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=420, train_size=0.80)\n","\n","#>>> Determine the shape of our training and testing sets.\n","print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)"]},{"cell_type":"code","execution_count":null,"id":"79f8d86b","metadata":{},"outputs":[],"source":["#>>> Display the balance of the dataset.\n","Counter(y_train)"]},{"cell_type":"code","execution_count":null,"id":"e304bca1","metadata":{},"outputs":[],"source":["#>>> Implement combination sampling wih SMOTEENN.\n","smote_enn = SMOTEENN(random_state=0)\n","X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n","\n","#>>> Display the balance of the dataset.\n","Counter(y_resampled)"]},{"cell_type":"code","execution_count":null,"id":"c2de9bf8","metadata":{},"outputs":[],"source":["#>>> Create a StandardScaler instance.\n","scaler = StandardScaler()\n","\n","#>>> Fit the Standard Scaler with the training data.\n","X_scaler = scaler.fit(X_train)\n","\n","#>>> Scale the data.\n","X_train_scaled = X_scaler.transform(X_train)\n","X_test_scaled = X_scaler.transform(X_test)"]},{"cell_type":"markdown","id":"664150d4","metadata":{},"source":["# AUTOVIZUALIZE DATA"]},{"cell_type":"code","execution_count":null,"id":"410e0fc2","metadata":{},"outputs":[],"source":["#>>> Initialize the Autoviz class in an object.\n","AV = AutoViz_Class()\n","\n","#>>> Must specify in order for AutoViz to display plots.\n","%matplotlib inline\n","\n","#>>> Passing the source data and parameters.\n","graph = AV.AutoViz(\n","    filename=\"\",\n","    save_plot_dir='./Images',\n","    sep=',',\n","    depVar='hazardous',\n","    dfte=neo_df,\n","    header=0,\n","    verbose=2,\n","    lowess=False,\n","    chart_format='jpg',\n","    max_rows_analyzed=1500000,\n","    max_cols_analyzed=30,\n",")"]},{"cell_type":"markdown","id":"5df989c4","metadata":{},"source":["# SUPPORT VECTOR MACHINE MODEL"]},{"cell_type":"code","execution_count":null,"id":"98829cac","metadata":{},"outputs":[],"source":["#>>> Create SVM model instance.\n","svm_model = SVC(kernel='rbf')\n","\n","#>>> Fit the data.\n","svm_model = svm_model.fit(X_train_scaled, y_train)\n","\n","#>>> Make predictions using the test data\n","svm_predictions = svm_model.predict(X_test_scaled)\n","\n","#>>> Calculate the confusion matrix.\n","svm_cm = confusion_matrix(y_test, svm_predictions)\n","\n","#>>> Create a DataFrame from the confusion matrix.\n","svm_cm_df = pd.DataFrame(\n","    svm_cm, index=['Actual Non-Hazardous', 'Actual Hazardous'], columns=['Predicted Non-Hazardous', 'Predicted Hazardous'])\n","\n","#>>> Display results.\n","print('Confusion Matrix')\n","display(svm_cm_df)\n","print(f'\\nAccuracy Score : {accuracy_score(y_test, svm_predictions):.3f}')\n","print('\\nClassification Report')\n","print(classification_report(y_test, svm_predictions))"]},{"cell_type":"markdown","id":"269384b3","metadata":{},"source":["# LOGISTIC REGRESSION MODEL"]},{"cell_type":"code","execution_count":null,"id":"27ddd2e0","metadata":{},"outputs":[],"source":["#>>> Create the LogisticRegressionCV instance.\n","log_model = LogisticRegressionCV()\n","\n","#>>> Fit the model.\n","log_model = log_model.fit(X_train_scaled, y_train)\n","\n","#>>> Make predictions using the testing data.\n","log_predictions = log_model.predict(X_test_scaled)\n","\n","#>>> Calculate the confusion matrix.\n","log_cm = confusion_matrix(y_test, log_predictions)\n","\n","#>>> Create a DataFrame from the confusion matrix.\n","log_cm_df = pd.DataFrame(\n","    log_cm, index=['Actual Non-Hazardous', 'Actual Hazardous'], columns=['Predicted Non-Hazardous', 'Predicted Hazardous'])\n","\n","#>>> Display results.\n","print('Confusion Matrix')\n","display(log_cm_df)\n","print(f'\\nAccuracy Score : {accuracy_score(y_test, log_predictions):.3f}')\n","print('\\nClassification Report')\n","print(classification_report(y_test, log_predictions))"]},{"cell_type":"markdown","id":"u-xTidEeOcai","metadata":{"id":"u-xTidEeOcai"},"source":["# DECISION TREE CLASSIFICATION"]},{"cell_type":"code","execution_count":null,"id":"Q0kSkjRDLQVz","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":112},"executionInfo":{"elapsed":423,"status":"ok","timestamp":1657897973444,"user":{"displayName":"Christopher Madden","userId":"02171637580083339973"},"user_tz":240},"id":"Q0kSkjRDLQVz","outputId":"228c7c62-e45c-4039-ba4a-da4c6866be20"},"outputs":[],"source":["#>>> Create the decision tree classifier instance.\n","dt_model = tree.DecisionTreeClassifier()\n","\n","#>>> Fit the model.\n","dt_model = dt_model.fit(X_train_scaled, y_train)\n","\n","#>>> Make predictions using the testing data.\n","dt_predictions = dt_model.predict(X_test_scaled)\n","\n","#>>> Calculate the confusion matrix.\n","dt_cm = confusion_matrix(y_test, dt_predictions)\n","\n","#>>> Create a DataFrame from the confusion matrix.\n","dt_cm_df = pd.DataFrame(\n","    dt_cm, index=['Actual Non-Hazardous', 'Actual Hazardous'], columns=['Predicted Non-Hazardous', 'Predicted Hazardous'])\n","\n","#>>> Display results.\n","print('Confusion Matrix')\n","display(dt_cm_df)\n","print(f'\\nAccuracy Score : {accuracy_score(y_test, dt_predictions):.3f}')\n","print('\\nClassification Report')\n","print(classification_report(y_test, dt_predictions))"]},{"cell_type":"code","execution_count":null,"id":"4ac2be1f","metadata":{},"outputs":[],"source":["#>>> Sort the features by importance.\n","sorted(zip(dt_model.feature_importances_, X.columns), reverse=True)"]},{"cell_type":"code","execution_count":null,"id":"l6ffjISuMEio","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1657897973448,"user":{"displayName":"Christopher Madden","userId":"02171637580083339973"},"user_tz":240},"id":"l6ffjISuMEio","outputId":"0518b4dd-4e42-45da-cf4e-3cd5ef657146"},"outputs":[],"source":["#>>> Display decision tree and save the output.\n","clf = tree.DecisionTreeClassifier(max_leaf_nodes=10, random_state=0)\n","clf.fit(X_train, y_train)\n","plt.figure(figsize=(18,18))\n","tree.plot_tree(clf, feature_names = ['est_diameter_min', 'est_diameter_max', 'relative_velocity', 'miss_distance', 'absolute magnitude'], class_names= ['non-hazardous', 'hazardous'], filled=True, rounded=True, proportion=True, node_ids=True, fontsize=14)\n","plt.savefig('./Images/decision_tree.jpg')"]},{"cell_type":"markdown","id":"adadc39e","metadata":{},"source":["# GRADIENT BOOSTED TREE"]},{"cell_type":"code","execution_count":null,"id":"02d63fab","metadata":{},"outputs":[],"source":["#>>> Create a classifier object.\n","learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n","for learning_rate in learning_rates:\n","    classifier = GradientBoostingClassifier(n_estimators=20,\n","                                            learning_rate=learning_rate,\n","                                            max_features=5,\n","                                            max_depth=3,\n","                                            random_state=0)\n","\n","    #>>> Fit the model.\n","    classifier.fit(X_train_scaled, y_train)\n","    print(\"Learning rate: \", learning_rate)\n","\n","    #>>> Score the model.\n","    print(\"Accuracy score (training): {0:.3f}\".format(\n","        classifier.score(\n","            X_train_scaled,\n","            y_train)))\n","    print(\"Accuracy score (validation): {0:.3f}\".format(\n","        classifier.score(\n","            X_test_scaled,\n","            y_test)))"]},{"cell_type":"code","execution_count":null,"id":"6b7915d9","metadata":{},"outputs":[],"source":["#>>> Choose a learning rate and create classifier.\n","gbt_model = GradientBoostingClassifier(n_estimators=20,\n","                                        learning_rate=0.75,\n","                                        max_features=5,\n","                                        max_depth=3,\n","                                        random_state=0)\n","\n","#>>> Fit the model.\n","gbt_model.fit(X_train_scaled, y_train)\n","\n","#>>> Make predictions using the testing data.\n","gbt_predictions = gbt_model.predict(X_test_scaled)\n","\n","#>>> Calculate the confusion matrix.\n","gbt_cm = confusion_matrix(y_test, gbt_predictions)\n","\n","#>>> Create a DataFrame from the confusion matrix.\n","gbt_cm_df = pd.DataFrame(\n","    gbt_cm, index=['Actual Non-Hazardous', 'Actual Hazardous'], columns=['Predicted Non-Hazardous', 'Predicted Hazardous'])\n","\n","#>>> Display results.\n","print('Confusion Matrix')\n","display(gbt_cm_df)\n","print(f'\\nAccuracy Score : {accuracy_score(y_test, gbt_predictions):.3f}')\n","print('\\nClassification Report')\n","print(classification_report(y_test, gbt_predictions))"]},{"cell_type":"code","execution_count":null,"id":"f034df56","metadata":{},"outputs":[],"source":["#>>> Sort the features by importance.\n","sorted(zip(gbt_model.feature_importances_, X.columns), reverse=True)"]},{"cell_type":"markdown","id":"edfd5f0d","metadata":{},"source":["# RANDOM FOREST CLASSIFIER"]},{"cell_type":"code","execution_count":null,"id":"993d8dcf","metadata":{},"outputs":[],"source":["#>>> Create a random forest classifier.\n","rf_model = RandomForestClassifier(n_estimators=128, random_state=78) \n","\n","#>>> Fit the model.\n","rf_model = rf_model.fit(X_train_scaled, y_train)\n","\n","#>>> Make predictions using the testing data.\n","rf_predictions = rf_model.predict(X_test_scaled)\n","\n","#>>> Calculate the confusion matrix.\n","rf_cm = confusion_matrix(y_test, rf_predictions)\n","\n","#>>> Create a DataFrame from the confusion matrix.\n","rf_cm_df = pd.DataFrame(\n","    rf_cm, index=['Actual Non-Hazardous', 'Actual Hazardous'], columns=['Predicted Non-Hazardous', 'Predicted Hazardous'])\n","\n","#>>> Display results.\n","print('Confusion Matrix')\n","display(rf_cm_df)\n","print(f'\\nAccuracy Score : {accuracy_score(y_test, rf_predictions):.3f}')\n","print('\\nClassification Report')\n","print(classification_report(y_test, rf_predictions))"]},{"cell_type":"code","execution_count":null,"id":"c8b814e8","metadata":{},"outputs":[],"source":["#>>> Sort the features by importance.\n","sorted(zip(rf_model.feature_importances_, X.columns), reverse=True)"]},{"cell_type":"markdown","id":"a65b34ea","metadata":{},"source":["# VOTING CLASSIFIER"]},{"cell_type":"code","execution_count":null,"id":"c3a6f239","metadata":{},"outputs":[],"source":["#>>> Create voting classifier.\n","eclf = VotingClassifier(estimators=[('Support Vector Machine', svm_model), ('Logistic Regression', log_model), ('Decision Tree', dt_model), ('Gradient Boosted Tree', gbt_model), ('Random Forests', rf_model)], voting='hard')\n","\n","#>>> Run voting classifier.\n","for clf, label in zip([svm_model, log_model, dt_model, gbt_model, rf_model, eclf], ['Support Vector Machine', 'Logistic Regression', 'Decision Tree', 'Gradient Boosted Tree', 'Random Forests', 'Ensemble']):\n","        scores = cross_val_score(clf, X, y, scoring='accuracy', cv=5)\n","        print(\"Accuracy: %0.2f (+/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))"]},{"cell_type":"markdown","id":"94bd3256","metadata":{},"source":["# DEPLOY MODELS TO TABLEAU"]},{"cell_type":"code","execution_count":null,"id":"f4854044","metadata":{},"outputs":[],"source":["#>>> Deploying models to tableau.\n","client.deploy('SVM', svm_model,'Support Vector Machine Model', override = True)\n","client.deploy('LRM', log_model,'Logistic Regression Model', override = True)\n","client.deploy('DTM', dt_model,'Decision Tree Model', override = True)\n","client.deploy('GBT', gbt_model,'Gradient Boosted Tree', override = True)\n","client.deploy('RFM', rf_model,'Random Forests Model', override = True)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"NEO_Analysis.ipynb","provenance":[]},"kernelspec":{"display_name":"PythonData","language":"python","name":"pythondata"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"nbformat":4,"nbformat_minor":5}
